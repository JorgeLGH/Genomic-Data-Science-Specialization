---
title: "Notes from Introduction to Genomic Technologies"
author: "Jorge Luis GÃ³mez"
date: "`r Sys.Date()`"
output:
  html_document: 
    fig_height: 8
    fig_width: 13
    number_section: yes
    theme: cerulean
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---
# Overview

## Why Genomics?

Genomics is the study of the genomes within the living organisms. Everyone and every living thing has a genome, but we're very similar even though we are not so different genomically speaking. Many traits of the living organisms are determined by the genomes, since our development is coded by the genome itself. The same code can make all the cells within a single genome, meaning every cell has the same instructions, yet they make different cells.

Important diseases, such as cancers, are directly related with the expression of the genome. Since they are dividing without control, there must be some mutations that can affect the regulation. The mutations occur during replication, age and different environmental causes. Some mutations affect genes, such as those that control cell division, that may induce the cancer.

We transcribe the exons from the genome itself, then we have RNA that will be translated into proteins; at least most of the times. The central dogma of molecular biology has since proven as faux. The information flow is actually more complex, since proteins can affect the genome, which thereby affects all the transcription and translation of the genes and proteins. Proteins can self regulate or regulate other proteins, meaning the genes affect proteins and viceversa. 

## Sequencing

We determine the mutations, the exons, the whole genomic data with sequencing. The bases per run now yield in the millions per run, while some years ago it was merely some couple hundreds. The whole genomics allow us to see and gather more information and create data sets from a myriad of sources. The cost has also decreased exponentially, so there can be more sequencing than ever before. The data is now stored in different archives and everyone has their own way of analyzing and storing, such as NCBI, TCGA, etc. 

## What is Genomics?

The study of genomes. The genome is the complete genomic information within the organism. The structure of a genome, is a chain of nucleotides; they are divided in chromosomes. In case of humans we have 23 pairs of chromosomes. The common structure is the centromere and the telomeres at the end. We want to know what the genome sequences, meaning all the functions and the products it may code for, such as proteins and tissues. We can also evaluate of the genomes' evolution throughout time. Most of the changes are small between generations among species, but we also share a big part of the chromosome information with all organisms since we have similar necessities, such as replication. The definition may also relate to genes, which are elements within the genomes that may or may not, code for products. 

We distinguish genomics from biology and genetics may be synthesized with the scope. Genomics focuses on all genes in a genome, yields large amounts of information and has lots of computational work for analysis. Genetics is more focused on a handful of genes and has low output of information and the analysis is not as laborious. 

## What is Genomic Data Science?

Is is comprised by biology, statistics and computer science. We start off with subjects or samples, from which we collect other samples, such as cells; we send them to the lab and them we send them to sequence it at certain labs. When we have the reads, if available, we align the reads to a reference genome and sort of tell the differences. We then publish the data and may also analyse with statistics and other methods. 

We have to start with our experimental design. Number of samples, sampling, sequencing, etc. When we align and assemble the genomes we can look for mutations, differences, presence, etc. Pre-processing and normalization is a fundamental step since we can have biases due to the technologies themselves such as the sequencing or the sampling. After, we use statistics and techniques such as machine learning to obtain more information, to actually gain knowledge and make conclusions. The development of software is an important part of data science and bioinformatics in it of itself. Population genomics is an interesting branch, as we can follow a group of subjects and understand, for example, why some traits are expressed or why some have some inclinations to have certain diseases. Integrative genomics collect different experiments and types of samples, measurements, and data that are integrated into actual representations that yield more insightful views of what is actually happening. 

# Molecular Biology

## Just Enough Cell Biology

At the most basic level, we have three domains, Bacteria, Eukaryota, and Archaea. Prokaryotes do not have nucleus, while us, eukaryotes, have them. In eukaryotes, the genetic information is more enclosed, as it is in the nucleus; with important exceptions that are the mitochondria and the chloroplasts. We have cell division due to the cell cycle (mitosis), which is composed multiple phases, ending with the division itself. When the mitosis is being carried out, the genetic information is doubled as both resulting cells have the same genetic information. Not all cells divide and multiply, but some differentiate and do not replicate by themselves, the stem cells are the ones that divide and then mature into different cell types that will have specific functions. 

When sexual reproduction occurs, the meiosis takes place. The cells will not have the same information as in the meiosis recombination occur and the progenitors' information is actually shuffled. The cells have different recombination, when reproduction occur, so the differences between sibling from the same parents have infinite possibilities of recombination and having different traits. 

## Important Molecules in Molecular Biology

Few of the key molecules will be talked about, not all, but some of the most critical molecules that will affect the genomic structure and expression. DNA is comprised of purines and pyrimidines (adenine and guanine; cytosine and thymine). The nucleotides bind A-T C-G, in this case we're only speaking of the DNA. If one strand is known, meaning the sequence of nucleotides, then the opposite strand is also known due to their complementary nature. Almost all genetic information is stored in DNA which is stored, in eukaryotes, in the nucleus. The sense is always read 5' to 3' (positive strand) (3' to 5' is the negative strand). When speaking of RNA, the difference is we have single strands, and the thymine is replaced with thymine. 

In the case of eukaryotes, the DNA is what is inherited from generation to generation and what actually has the mutations and recombination that will make the individuals unique. The proteins, made from amino acids, will have a variety of functions that will be used in the organism. The RNA will be translated into the amino acids that will be stringed into the protein. We have 20 amino acids and a particular order of nucleotides in the RNA molecule code for a stop codon. All coding RNA is read in codons of 3 nucleotides. There are 64 codons, 3 are the stop codons. 

## The Human Genome Project 

A groundbraking project that started in the late 90's. It was the *Manhattan project* of biology and gained momentum and influence with many scientists. It started in 1989 with NIH's and DOE's support as well as the Sanger Centre in England. It had a goal of 3 billion pasepairs and was a cost of 10 dollars to 1 dollar per base and had a goal to end by 2005. People were opposed as most of the DNA does not code for any protein and people thought it was a waste of time.

In the early 90's the idea was taking chunks of DNA and put them into BACs for sequencing later. The problem was aligning and using the BACs 'cause of the nature of identifying maps to locate the genes in the genome. In 1995, TIGR sequenced the first bacterial genome ever with whole genome sequencing *shotgun*; so in 1998 the race began with Applied Biosystems and Craig Venter and formed Celera Genomics in order to sequence the human genome to get patent from the genomes. Celera would announce they would finish by 2001, as well as the public efforts would publish a *draft* of the genome in the same year; in the 2000 NIH and Celera talked about publishing independently but it never was like that, so everyone *finished* at the same time. 

The genome was to identify all genes, their sequences and try to improve humans' quality of life. The project gave an approximate of the number of human genes, the result was around 30,000-40,000 genes. The estimate was an approximate with wide range of discrepancy. 

## Molecular Biology Structures

The DNA and the cell has structures that help us understand how the cell is governed. The DNA itself is very compressed in histones, or similar proteins, so that the DNA is coiled that will eventually form the chromosomes. When translated, it will relax. We can have repeats of identical sequences in tandem in the same chromosome, or it can be interspersed where the repeats can be dispersed along the chromosome. The structure of a typical mRNA has a cap in the head, a portion of untranslated regions at the 5' and 3' ends respectively, then the coding sequences and finally a poly-a tail that means it is matured. The RNA that results from the DNA may have introns before the maturation, therefore different isoforms of the same splicing of the protein can give different combinations. 

Proteins have structures, they are composed of long chains of amino acids, so that they form different structures. The secondary structure can be helices and sheets. The tertiary structure conforms a 3D structure given by combinations of long chains of amino acids; finally, the quaternary structure is the combination of at least two tertiary structures, giving an even more complex molecule. 

The existence of transcription factors is very important. These determine the behavior of the cells, at least partly, meaning these genes and their protein products can bind to the genes, *manually* regulation the expression of proteins, even themselves. Epigenetics are factors outside the genetic material itself, such as DNA tagged with methylation that can be inherited in cell division, meaning it is not a mutation nor a transcription factor, but can regulate the expression of the genes.

## From Genes to Phenotypes

The genotype is the collect of all the sequences of the genes, which determine how the organism will function and certain traits; phenotype is something that can be *seen* in the organism, other than genotype, and it is, in part, determined by the genotype. A clear example to show the genotype is Mendel's experiments with recessive and dominant genes in certain strains of peas. Different organisms, even from the same species, may have certain mutations or genetic variances that may be present according to geographical locations. Particular traits, or phenotypes, may be sequenced so that the genotype of interesting diseases or phenomena can be analysed and will be checked out for mutations or specific sequences associated with the traits. 

# Measurement Technology

## PCR (Polymerase Chain Reaction)

Remarkable and powerful way of making many copies of the sample DNA. We use a considerable amount of sample DNA for sequencing, so it is important to make as much as possible. The beginning is to denature the DNA and make the primers that are complimentary with our DNA strands. The DNA denature is done in the melting process by heating the sample, when the anneal comes, the temperature is a little lower and the primers attach; in this step is when the DNA polymerases come into action and starts *filling* the gaps. The result of the first round would yield double the DNA amount, it is important to note that, for example, dNTPs are added before using the *termociclador*. Normally we repeat the process a few dozen times. 

## Next Generation Sequencing

Latest sequencing technologies. It started with Fred Sanger's invention, which was in the 90's, then it came the second generation around 2007 and we have the third generation and single molecule sequencing since the 2010's. All rely on taking your sample, copying the sample and sequence. Many methods exist, but the most used NGS is with sequencing by synthesis. 

We start with our DNA fragments attached to the slide, where PCR takes place and generate even more strands of the samples (PCR then denature and just leave the copies of the original). After, we add labeled nucleotides that emit fluorescence with certain laser excitation. The chemically modified polymerases can only add one nucleotide and the fall off, then it is activated again and it can add another nucleotide and so on until we have the reads of the reads. Errors, which increase the later in the cycles, can be such as getting behind or ahead in the reaction, so the molecules show different colors (out of sync). 

## Applications of Sequencing

We can ask different questions with sequencing, it is now possible due to price and accessibility. Some applications are, for example, exon sequencing (coding sequences that are concatenated and will eventually be translated into proteins). We take DNA molecules and can sequence just a little percentage, sch as the exome, it will be fragmented as always and attached to the sequencing slides with probes and will be sequenced. 

RNAseq is used to observe the *turned on* genes that are being expressed in a certain condition or organism. After transcription, the polyA chain at the end of mature RNA is used as probe in the slide to attach, then, usually, it is retrotranscribed into complementary DNA and then sequence the complimentary strand that is being synthesized.

ChiPseq was used to cross-link proteins that attach as transcription factors, the molecule is fragmented and we use antibodies that select the proteins in order to *pull down* the proteins; after, we separate the DNA that was attached to the protein and sequence those fragments, yielding promoters or sequences that act as anchors for protein regulation. 

Methylseq is to identify where on the genome, the sequences have methylation. It is an important feature as it is epigenetics. We first split DNA into two aliquots/samples, and then you treat one so that the sequences' cytokines that are not methylated are turned into uracil; then we sequence both and align. 

# Computing Technology

## What is Computer Science

Comprises different activities, we can divide them by theory, systems and applications. When speaking of theory, we speak of what computers can theoretically do. By systems we study the computers, the software, the OS, etc. Applications relate to everything else. We must think computationally, so we need to understand the computer, what it can do and what we can describe for it to do. One of the topics are operating systems, such as Unix and Linux; it is like this due to the nature of the OS, as it manipulates data and process all our programs and data. Programming languages are also of great importance, as they *how we talk to computers*, they define certain terms and meanings so that we tell the computers what to do. Engineering is hard, as coding is not sufficient, it has to be tested in different cases and must work as it is intended no matter the differences. Devices refer to the physical aspects, the hardware; they are controlled bu computers or programs that are controlled with computer science. 

## Algorithms

They are instructions, basically, of doing something. We do not necessarily need a computer to create and execute an algorithm; they described with great detail a step by step process. For example, we can find maximum values from data, sort data by specific criteria. We start with different data types and start acting upon the given information and the set of instructions. Efficiency of algorithms is important, as the amount of data troubles the algorithm; we seek faster algorithms with same results. 

## Memory and Data Structures

Data structures allow us to store data for later processing. Example: we have sequences and get them into memory to store them. We store in efficient ways, we store what is useful in order to be more available to retrieve them later and use them. We store as to keep available and identifiable. We can store in many ways, but normally it is classified and stored in *trees*, *links*, etc. Every piece of information has an address, or pointers, that will be a specific ID so that once we are in the correct place, we can access it. We store with binary language, every piece of text will be represented with values of bytes and bits. When representing DNA, we can use 4 binary combinations, meaning 2 bits to represent each letter of the nucleotides; **A=00**, **c=01**, **G=10**, **T=11**. We can check for patterns in the DNA as to search, for example, for introns and exons; we search for patterns as to generate probabilities for further analysis and later fact checking with real data. 

## Efficiency

It is a key topic in computer science, as big data need to be as fast as it can be in order to actually yield information. We must make as efficient as possible all the algorithms as to get as much information as it possibly can. The idea is to use as less time as it is possible without affecting the performance. We seek to do the same in less time and steps. 

## Software Engineering

It is important as it is critical to analyse the data, how reliable it can be and if it is performing as good as we want. We must know and understand the logic behind the program we are executing, how all the different cases can or cannot be handled with the code at hand. Data is so variable, the software must be robust to withstand different conditions and bid data sets. We must understand the logic as to avoid putting the data into *black boxes*. We can trust software, but we must verify the results, specially with bigger data sets. Not because the program didn't crash, it means it is free of bugs and always correct.

## What is Computational Biology Software

We use it to transform raw data into useful information. We gather data from genomic experiments, specially sequence data. The software transform the reads and sequences, into information and actually help us interpret the raw data that otherwise wouldn't be useful at all. We use work pipelines in order to transform raw data into condensed data from which we draw out our conclusions. The process varies depending on the pipeline, it can be condensed, align, separate, filter, etc. There are hundreds of pipelines, all is designed in order to extract specific information of interest. 

An example is RNA-seq. We take RNA from a cell or cells in order to detect which genes are being turned on and which aren't. We extract RNA and make them into sequences and reads from the sample. The pipeline *Tuxedo* uses `Bowtie2`, `TopHat2`, `Cufflinks`, and `Cuffdiff2`. We start with RNA reads
and align them to the reference genome (it can be aligned to different exons without introns), assemble them into the original genes and then infer the expression levels of each gene; then you compare it between different data sets and get information. Pipelines get updated and worked on, so it can be better; the *Tuxedo* pipeline has also been updated and uses different algorithms and tools, it is of utter importance to understand the software and keep up to date with the newest and bet tools. Software changes with time, specially now, the differences may be abysmal when comparing the newer generations; either for sequencing or analyzing. Even the data generated by sequencing is changing, so the analysis must do so as well. 

# Data Science Technology

## Why Care About Statistics

It is important when considering genomic data science. It is possible to predict even chemotherapy for personalized therapies with statistic analysis; but it is important due to the veracity of the results that can be yield. We must care about statistics, the nature of results actually has very important implications, even in genomics and data science in every field. We must change the perspective and emphasize that statistics are of utmost importance for science.

## What went Wrong?

In the example of Duke University, where genomic data was used to infer the best possible chemotherapies for the patients, something went wrong. The first, is that the data and the code that was used, was not shared with the rest of the community; is has to be reproducible, but in this case, that wasn't true. Then, the lack of cooperation from the people who were in charge for others to replicate the analysis. The lack of expertise in statistics was also important, as they used silly prediction rules and were false, meaning the model was wrong from the start. The study design was also at fault, with batch effects, meaning the experimental design was initially wrong. The predictions weren't locked down, meaning their predictions were faulty, as the algorithm yielded different results given the day and run of the analysis. 

## The Central Dogma of Statistics

Statistical inference has a central dogma too. Taking the full populations into account most times is not possible, so we use probability to take a sample from the population, and we'd like for it to represent the total population. We the use inference to determine certain traits from our population. How do we quantify the variability from our samples? It is central to inference in statistics. The samples may no longer represent the populations when we try to make our inference. 

## Data Sharing Plans

We must be able to share data with our peers and anyone interested. A data set is composed of four components. 

1. The raw data: It is often the raw sequence reads; it mustn't have changes of any kind.
2. A tidy set: It has been done some processing for easier access. It has variables in columns, the observations as rows, one table per kind of variable and linking indicators for the columns.
3. A code book describing each variable and its values in the tidy data set: It must have descriptions like variable names, descriptions, specifications of the variables, variable units and the study's quirks.
4. An explicit and exact recipe you used to go from 1->2->3: We use a script to create the tidy data.

Without all these components, we can't give precise information to share to others.

## Getting help with statistics

How can we get help in statistics? Learn it! We can learn with online resources such as this course or many other classes. We can use `StackOverflow` and `Cross Validated` to answer specific questions with online communities. We can also hire bioinformaticians, but long term collaborations are better. Some centers have specific departments for this sort of problems and topics. 

## Plotting your Data

Most analysis will have plots in order to visualize and interact with the results and data. We make big data as small as possible and summarize them in order not to lose information, and also transmit the message we want to share. We take the summaries from the data in order to discover certain characteristics from our data. We want for a plot to give as much as information in a clear manner. Graphs that look cool but do not tell much should be avoided. 

## Sample Size and Variability

Experimental design must consider the sample size and the variability of our representative samples that try to describe the total population. The sample size formula regarding money, is as follows $N=\frac{money}{price/measure}$. In reality, we use the power of our samples to determine the probability that if there is a true difference that can be observed in the data set, it will be detected. It depends, for example, on the size of the samples, the variance of each sample (population), and the effect we want to determine. We can determine this, for example, within two groups and with the help of the script. 

```{r}
# in this example, we see that with 10 replicates in each sample, with these parameters, there's only an 18% chance of detecting the effect
power.t.test(n=10, delta = 5, sd=10)

# we can also define it by the power we want
power.t.test(delta=5, sd=10, power = 0.8)

# we can use more information to determine the sizes
power.t.test(delta = 5, sd=10, power = 0.8, alternative = "one.sided")
```

The power is a curve, meaning the potential sizes of sample size will have different effects on the power, meaning, the bigger the effect, the easier it gets to detect. A power calculation is hypothetical, as the effect is just imagined, not measured.

The variability in genomic measurement can be described in three measurements.

- Phenotypic variability
- Measurement error
- Natural biological variation

There is often a rush when new technologies appear to say they're better and less variable, but that is not always the case. We must always check for the variation within our sample and the natural variation in the populations. 

## Statistical Significance

The idea is to know if observed differences in the samples are *real*. We imply there is a difference in the two, or more, sampled groups; for sample, to compare the mean values of expression in certain genes. To determine a *real*, or significant difference, is the *t-statistic*, which is defined as follows. 

$$
\frac{\bar{Y}-\bar{X}}{\sqrt{\frac{S^2_Y}{N}+\frac{S^2_X}{M}}}
$$
The average of the *Y* values minus the average value of *X* divided by the estimates of the variables of both variables by units of variability. The bigger the difference, it is more likely to exist a real difference. 
The most common approach is the *p-value*. First, we compare and generate the t-statistic for a sample, the use a permutation test. We randomly scramble the data and calculate the statistic in each iteration, so we get all the statistics from all the random calculations. Then, we compare the observed and the values from all the iterations, we only care for differences, so we check for how many times our scrambled data was bigger than the one we observed.

**P-value:**
The probability of observing a statistic that extreme if the null hypothesis is true.

**It is not**
- Probability the null is true
- Probability the alternative is true
- A measure of statistical evidence

## Multiple Testing

The tests are faulty by design, since the measure for the t-test is prone to error with each statistic you try to prove. To prevent such false positives, we use error rates. The most common ones are *family wise error rate* and *False discovery rate*. The first one is considered that if we are trying to test many hypothesis, at least one will yield a false positive. The second one is the expected number of false positives, divided by the total discoveries; so it quantifies the error rates of the discoveries. We must interpret each error differently and with different approaches.  

## Study Design, Batch Effects, and Confounding

Confounding, can show correlations that would try to explain certain effects, but in reality is not actually related. The variables will appear as they have an effect, while the meaningful variables that are correlated and latent, do not get reported but still have the effect. The most common confounding is the **batch effect**.

The batch effect can be, for example, when you take samples at different times. There is a confounder with the dates, for example the technology can change, the supplies, the equipment, etc. We can deal with confounders with certain tools such as randomization of the variables. We can also use stratification around known confounders, for example, gender or age, we can use the treated and control groups with combined stratification. It is better to have a balanced, referring sample size, experiment; have many replicas and use control groups. 




